from dotenv import load_dotenv

load_dotenv()

import os
import json
import subprocess
import re
import uuid
import logging
from typing import TypedDict, Annotated, List, Dict, Optional, Literal, Any
import datetime
import sys

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from langgraph.graph import StateGraph, END, MessagesState
from langgraph.graph.message import add_messages
from langchain_core.messages import (
    HumanMessage,
    AIMessage,
    SystemMessage,
    BaseMessage,
)
from langchain_core.messages.tool import ToolCall
from pydantic import BaseModel, Field
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI  # Or your preferred LLM provider
from langchain_ollama import ChatOllama  # Ollama LLM provider
from mem0 import MemoryClient
from qdrant_client import QdrantClient, models as qdrant_models
from services.ollama_service import get_embedding


# --- Setup Logging ---
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- Configuration ---
# Set these environment variables
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# MEM0_API_KEY = os.getenv("MEM0_API_KEY")
# QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
# QDRANT_API_KEY = os.getenv("QDRANT_API_KEY") # Optional

# --- Constants ---
PENTEST_MEM0_COLLECTION = "web_pentest_knowledge_base"
QDRANT_COLLECTION_VULNS = "pentest_vulnerabilities"
QDRANT_COLLECTION_PAYLOADS = "pentest_payloads"
QDRANT_COLLECTION_REPORTS = "pentest_past_reports"

# --- Memory Management (Adapted from your provided examples) ---


class PentestMemoryClient:
    def __init__(self, collection_name: str = "web_pentest_memories"):
        self.collection = collection_name
        if os.environ.get("MEM0_API_KEY") is not None:
            os.environ["MEM0_API_KEY"] = os.environ.get("MEM0_API_KEY")
        if os.environ.get("OPENAI_API_KEY") is not None:
            os.environ["OPENAI_API_KEY"] = os.environ.get("OPENAI_API_KEY")
        self.api_key = os.getenv("MEM0_API_KEY")
        if self.api_key:
            self.client = MemoryClient()
        else:
            self.client = None
            logger.warning(
                "MEM0_API_KEY not set. PentestMemoryClient will not persist memory."
            )

    def add(self, messages: list, user_id: str, session_id: str, metadata: dict = None):
        if not self.client:
            logger.warning(
                "Skipping add memory: Mem0 client not available or API key missing."
            )
            return "Memory not saved (client/key unavailable)."
        formatted = []
        for msg in messages:
            if hasattr(msg, "content"):
                if isinstance(msg, AIMessage):
                    role = "assistant"
                elif isinstance(msg, SystemMessage):
                    role = "system"
                else:
                    role = "user"
                formatted.append({"role": role, "content": msg.content})
        if not formatted:
            logger.info("No messages to add to memory.")
            return "No messages to save."
        try:
            response = self.client.add(
                formatted,
                user_id=user_id,
                session_id=session_id,
                collection=self.collection,
                version="v2",
                metadata=metadata or {},
            )
            logger.info(
                f"Memory added successfully via v2 format. Response: {response}"
            )
            return "Memory saved (v2)."
        except Exception as e:
            logger.error(
                f"Error adding memory (v2) for user {user_id}, session {session_id}: {e}"
            )
            return f"Error saving memory (v2): {e}"

    def search(self, query: str, user_id: str, session_id: str, limit: int = 3):
        if not self.client:
            logger.warning(
                "Skipping search memory: Mem0 client not available or API key missing."
            )
            return None
        try:
            results = self.client.search(
                query=query,
                user_id=user_id,
                session_id=session_id,
                collection=self.collection,
                limit=limit,
            )
            logger.info(f"Memory search raw results: {results}")
            if results:
                return "\n".join([m.get("memory", "") for m in results])
            else:
                logger.info("No relevant memories found.")
                return None
        except Exception as e:
            logger.error(
                f"Error searching memory for user {user_id}, session {session_id}: {e}"
            )
            return None


class QdrantManager:
    """Manages interaction with Qdrant Vector Store using Nomic/Ollama embeddings."""

    def __init__(
        self,
        url: str = None,
        api_key: str = None,
    ):
        self.url = url or os.getenv("QDRANT_URL")
        self.api_key = api_key or os.getenv("QDRANT_API_KEY")
        self.client = QdrantClient(url=self.url, api_key=self.api_key)
        self._ensure_collections_exist()

    def _ensure_collections_exist(self):
        collections_to_ensure = {
            QDRANT_COLLECTION_VULNS: "Known vulnerability signatures and details",
            QDRANT_COLLECTION_PAYLOADS: "Common payloads and exploits",
            QDRANT_COLLECTION_REPORTS: "Historical pentest data/report summaries",
        }
        try:
            existing_collections = [
                col.name for col in self.client.get_collections().collections
            ]
            for col_name, desc in collections_to_ensure.items():
                if col_name not in existing_collections:
                    if not self.client.collection_exists(collection_name=col_name):
                        self.client.create_collection(
                            collection_name=col_name,
                            vectors_config=qdrant_models.VectorParams(
                                size=768, distance=qdrant_models.Distance.COSINE
                            ),
                        )
        except Exception as e:
            logger.error(f"Error ensuring Qdrant collections exist: {e}")

    def add_entry(
        self,
        collection_name: str,
        text_content: str,
        metadata: Optional[Dict] = None,
        entry_id: Optional[str] = None,
    ):
        if not self.client:
            logger.warning(
                f"Qdrant client not available. Cannot add entry to {collection_name}."
            )
            return
        if metadata is None:
            metadata = {}
        metadata["text_content"] = text_content
        embedding = get_embedding(text_content)
        final_id = entry_id or str(uuid.uuid4())
        try:
            self.client.upsert(
                collection_name=collection_name,
                points=[
                    qdrant_models.PointStruct(
                        id=final_id, vector=embedding, payload=metadata
                    )
                ],
            )
            logger.info(
                f"[Qdrant] Added entry to {collection_name} (ID: {final_id}): {text_content[:50]}..."
            )
        except Exception as e:
            logger.error(
                f"Error adding entry to Qdrant collection {collection_name}: {e}"
            )

    def search_entries(
        self, collection_name: str, query_text: str, limit: int = 5
    ) -> List[Dict]:
        if not self.client:
            logger.warning(
                f"Qdrant client not available. Cannot search {collection_name}."
            )
            return []
        query_embedding = get_embedding(query_text)
        try:
            search_result = self.client.search(
                collection_name=collection_name,
                query_vector=query_embedding,
                limit=limit,
                with_payload=True,
            )
            results = [
                {"id": hit.id, "score": hit.score, "payload": hit.payload}
                for hit in search_result
            ]
            logger.info(
                f"[Qdrant] Searched {collection_name} for '{query_text}', found {len(results)} results."
            )
            return results
        except Exception as e:
            logger.error(f"Error searching Qdrant collection {collection_name}: {e}")
            return []


mem0_storage = PentestMemoryClient(collection_name="web_pentest_memories")
qdrant_db = QdrantManager()


# --- Agent State Definition ---
class TargetInfo(BaseModel):
    domain: Optional[str] = None
    ip_addresses: List[str] = Field(default_factory=list)
    subdomains: List[str] = Field(default_factory=list)
    technologies: List[str] = Field(default_factory=list)


class VulnerabilityInfo(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str
    description: str
    severity: Optional[
        Literal["Critical", "High", "Medium", "Low", "Informational"]
    ] = None
    target_component: Optional[str] = None  # e.g., specific URL, parameter
    scanner_tool: Optional[str] = None
    raw_output: Optional[str] = None  # Store raw tool output if relevant


class AccessPointInfo(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    method: str  # e.g., SQLi, RCE via CVE-XXXX
    target: str  # e.g., vulnerable URL, compromised host
    credentials_obtained: Optional[str] = None
    shell_url: Optional[str] = None


class PersistenceMechanismInfo(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    type: str  # e.g., web shell, new user account, cron job
    details: str  # e.g., shell path, username/password
    location: str


class WebPentestState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    user_id: str
    session_id: str  # Also used as thread_id for LangGraph

    target_info: TargetInfo
    recon_findings: Dict[str, Any]  # Free-form for now, can be more structured
    identified_vulnerabilities: List[VulnerabilityInfo]
    gained_access_points: List[AccessPointInfo]
    persistence_mechanisms: List[PersistenceMechanismInfo]

    current_phase: Literal[
        "initialization",
        "planning_recon",
        "scanning",
        "gaining_access",
        "maintaining_access",
        "analysis_reporting",
        "completed",
    ]
    phase_objective: Optional[str]

    # For tool interaction and user input
    pending_tool_guidance: Optional[str]  # If set, agent needs to provide this to user
    requires_user_input_for_tool: Optional[str]  # Name of tool awaiting user input
    last_tool_raw_output: Optional[str]

    # For final report
    report_data_summary: Optional[str]

    # Timeline of all actions/events
    timeline: List[
        Dict
    ]  # Each event is a dict with tool, phase, result, timestamp, etc.


# --- Pentesting Tools (as Pydantic models for LangChain tool usage by LLM) ---
# These will be called by the agent. Some are automated, some guide the user.


# Helper for CLI commands
def run_cli_command(command: str, timeout: int = 60) -> str:
    try:
        logger.info(f"Executing CLI: {command}")
        process = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            timeout=timeout,
            check=False,
        )
        if process.returncode == 0:
            return process.stdout.strip()
        else:
            return f"Error (code {process.returncode}): {process.stderr.strip()}"
    except subprocess.TimeoutExpired:
        logger.error(f"Command timed out: {command}")
        return "Error: Command timed out."
    except Exception as e:
        logger.error(f"Error executing command '{command}': {e}")
        return f"Error: {e}"


class WhoisInput(BaseModel):
    domain: str = Field(
        description="The domain name to perform a WHOIS lookup on (e.g., example.com)."
    )


@tool("whois_lookup", args_schema=WhoisInput)
def whois_lookup_tool(domain: str) -> str:
    """Performs a WHOIS lookup on a domain to find registration information."""
    return run_cli_command(f"whois {domain}")


class NslookupInput(BaseModel):
    domain: str = Field(
        description="The domain name for DNS enumeration (e.g., example.com)."
    )


@tool("nslookup_tool", args_schema=NslookupInput)
def nslookup_tool(domain: str) -> str:
    """Performs DNS enumeration (e.g., finding IP addresses) for a domain."""
    return run_cli_command(f"nslookup {domain}")


class NmapScanInput(BaseModel):
    target: str = Field(description="The IP address or hostname to scan with Nmap.")
    flags: Optional[str] = Field(
        description="Optional Nmap flags (e.g., -sV -T4 -p-). Default is a common safe scan.",
        default="-sV -T4",
    )


@tool("nmap_scan_tool", args_schema=NmapScanInput)
def nmap_scan_tool(target: str, flags: Optional[str] = "-sV -T4") -> str:
    """Discovers open ports and services on a target using Nmap."""
    return run_cli_command(f"nmap {flags} {target}")


class SearchQdrantPayloadsInput(BaseModel):
    query: str = Field(
        description="Description of the vulnerability or desired payload type to search for."
    )


@tool("search_qdrant_payloads", args_schema=SearchQdrantPayloadsInput)
def search_qdrant_payloads_tool(query: str) -> List[Dict]:
    """Searches the Qdrant vector store for relevant payloads or exploits based on a query."""
    if not qdrant_db or not qdrant_db.client:  # type: ignore
        return [{"error": "Qdrant client not available."}]
    return qdrant_db.search_entries(QDRANT_COLLECTION_PAYLOADS, query, limit=3)


# Guidance Tools (these will set state to require user input)
class GuideToolInput(BaseModel):
    target_url_or_info: str = Field(
        description="The target URL or relevant information for the tool."
    )
    additional_instructions: Optional[str] = Field(
        description="Any specific focus for the manual tool usage."
    )


# These tools don't return results directly but set flags in the state
# The LLM will interpret their output as a need to pause and ask the user.
@tool("guide_owasp_zap_scan", args_schema=GuideToolInput)
def guide_owasp_zap_scan_tool(
    target_url_or_info: str, additional_instructions: Optional[str] = None
) -> str:
    """
    Instructs the user to perform an OWASP ZAP scan on the target.
    The agent will then ask for the summary of findings.
    """
    guidance = (
        f"USER ACTION REQUIRED: Please perform an OWASP ZAP scan on '{target_url_or_info}'.\n"
        f"1. Open OWASP ZAP.\n"
        f"2. Use 'Quick Start' with URL: {target_url_or_info}, then click 'Attack'.\n"
        f"3. For more control, manually explore the site with ZAP as a proxy, then run Active Scan.\n"
    )
    if additional_instructions:
        guidance += f"4. Specific focus: {additional_instructions}\n"
    guidance += "5. After completion, review the 'Alerts' tab. Summarize key vulnerabilities found (name, severity, affected URL/parameter) and provide them back to me."
    return f"USER_GUIDANCE_PROVIDED_FOR::OWASP_ZAP::{guidance}"


@tool("guide_metasploit_exploitation", args_schema=GuideToolInput)  # Simplified input
def guide_metasploit_exploitation_tool(
    target_url_or_info: str, additional_instructions: Optional[str] = None
) -> str:
    """
    Instructs the user on using Metasploit for a potential vulnerability.
    The agent will then ask for the outcome.
    """
    guidance = (
        f"USER ACTION REQUIRED: Attempt exploitation using Metasploit based on information: '{target_url_or_info}'.\n"
        f"1. Launch `msfconsole`.\n"
        f"2. Search for relevant exploits (e.g., `search type:exploit platform:linux Apache Struts`).\n"
        f"3. Select an exploit: `use exploit/path/to/module`.\n"
        f"4. Set options: `set RHOSTS <target_ip>`, `set LHOST <your_ip>`, payload etc.\n"
    )
    if additional_instructions:
        guidance += f"5. Specific guidance: {additional_instructions}\n"
    guidance += "6. Run `exploit` or `run`.\n7. Report back the outcome: success (e.g., shell obtained, data exfiltrated) or failure, and any relevant details (exploit used, session ID)."
    return f"USER_GUIDANCE_PROVIDED_FOR::METASPLOIT::{guidance}"


# Tool list for the agent
pentest_tools = [
    whois_lookup_tool,
    nslookup_tool,
    nmap_scan_tool,
    guide_owasp_zap_scan_tool,
    guide_metasploit_exploitation_tool,
    search_qdrant_payloads_tool,
]
tool_map = {t.name: t for t in pentest_tools}


# --- Main Agent Class ---
class WebPentestAgent:
    def __init__(
        self,
        user_id: str,
        session_id: str,
        target_domain: Optional[str] = None,
        use_openai: bool = False,
    ):
        self.user_id = user_id
        self.session_id = session_id
        self.llm = self.setup_model(use_openai)

        self.graph = self._build_graph()
        self.agent_runnable = self.graph.compile(
            checkpointer=None
        )  # Add checkpointer for persistence later

        logger.info(
            f"WebPentestAgent initialized for user '{user_id}', session '{session_id}'."
        )
        # Initial state can be set here or via an entry point in the graph
        self.initial_state = WebPentestState(
            messages=[],
            user_id=user_id,
            session_id=session_id,
            target_info=(
                TargetInfo(domain=target_domain) if target_domain else TargetInfo()
            ),
            recon_findings={},
            identified_vulnerabilities=[],
            gained_access_points=[],
            persistence_mechanisms=[],
            current_phase="initialization",
            phase_objective=None,
            pending_tool_guidance=None,
            requires_user_input_for_tool=None,
            last_tool_raw_output=None,
            report_data_summary=None,
            timeline=[],
        )
        self.tool_failures = {}  # Track tool failures per session

    def setup_model(self, use_openai: bool = False):
        """Set up the LLM based on configuration (OpenAI or Ollama)."""
        if use_openai and os.environ.get("OPENAI_API_KEY"):
            model = os.getenv("OPENAI_MODEL_NAME", "gpt-4o")
            logger.info(f"Using OpenAI model: {model}")
            return ChatOpenAI(
                api_key=os.environ.get("OPENAI_API_KEY"),
                model=model,
                temperature=0.1,
                streaming=True,
            )
        else:
            model = os.getenv("OLLAMA_MODEL_NAME", "qwen2.5:7b")
            logger.info(f"Using Ollama model: {model}")
            return ChatOllama(
                model=model,
                temperature=0.1,
                base_url="http://localhost:11434",
            )

    def _get_phase_prompt_and_tools(
        self, state: WebPentestState
    ) -> tuple[str, List[BaseModel]]:
        phase = state["current_phase"]
        target_summary = f"Target: Domain={state['target_info'].domain}, IPs={state['target_info'].ip_addresses}"

        # Simplified available tools for this example. In reality, map tools to phases.
        available_tools_for_llm = pentest_tools

        if phase == "initialization":
            objective = "Determine the primary target (domain/IP) if not already specified, then transition to Planning & Reconnaissance."
            # No specific tools, LLM should ask user for target if needed.
            available_tools_for_llm = []  # No tools, just conversational
        elif phase == "planning_recon":
            objective = "Gather initial information about the target (DNS, WHOIS, subdomains, technologies). Store findings."
            target_summary += (
                f"\n Recon Findings so far: {str(state['recon_findings'])[:200]}"
            )
            available_tools_for_llm = [
                whois_lookup_tool,
                nslookup_tool,
            ]  # Add theHarvester, BuiltWith later
        elif phase == "scanning":
            objective = "Identify vulnerabilities using automated scanners and prepare for manual checks. Log all findings."
            target_summary += (
                f"\n Identified Technologies: {state['target_info'].technologies}"
            )
            available_tools_for_llm = [
                nmap_scan_tool,
                guide_owasp_zap_scan_tool,
            ]  # Add Nikto, Burp guidance
        elif phase == "gaining_access":
            objective = "Exploit identified vulnerabilities to gain unauthorized access. Prioritize high-impact vulnerabilities."
            target_summary += f"\n Key Vulnerabilities: {[v.name for v in state['identified_vulnerabilities'][:3]]}"
            available_tools_for_llm = [
                guide_metasploit_exploitation_tool,
                search_qdrant_payloads_tool,
            ]  # Add sqlmap etc.
        elif phase == "maintaining_access":
            objective = "Establish persistent access if required by engagement scope."
            target_summary += f"\n Access Points: {[ap.method for ap in state['gained_access_points']]}"
            # Add tools like Weevely guidance, etc.
        elif phase == "analysis_reporting":
            objective = (
                "Aggregate all findings and generate a comprehensive pentest report."
            )
            # Tool to compile report from state
        else:  # completed or undefined
            objective = "The penetration test is completed or in an undefined state."
            available_tools_for_llm = []

        system_prompt = f"""You are a Web Penetration Testing Agent.
Current Phase: {phase.upper()}
Objective for this phase: {objective}
{target_summary}

Valid phase names (for transitions): initialization, planning_recon, scanning, gaining_access, maintaining_access, analysis_reporting, completed

Available tools are functions you can call. Respond with a JSON list of tool calls if you need to use a tool.
Example: `[{{\"tool_name\": \"tool_name\", \"tool_input\": {{\"arg1\": \"value1\"}}}}]`
If you are guiding the user (e.g., for ZAP, Metasploit), the special tool output 'USER_GUIDANCE_PROVIDED_FOR::TOOL_NAME::details...' will be handled.
If you need to ask the user a question, or provide a summary, respond with text directly.
If the current phase objective is complete, state \"PHASE_OBJECTIVE_COMPLETE: [Next Phase Name, e.g., scanning]\" to transition.
If all pentest objectives are met, state \"PENTEST_COMPLETE\".

Current findings:
Recon: {str(state['recon_findings'])[:300]}
Vulnerabilities: {str([v.model_dump(exclude_none=True) for v in state['identified_vulnerabilities']])[:300]}
Access Points: {str([ap.model_dump(exclude_none=True) for ap in state['gained_access_points']])[:300]}

What is your next action or question?
"""
        return system_prompt, available_tools_for_llm

    # --- Graph Nodes ---
    def initial_setup_node(self, state: WebPentestState) -> WebPentestState:
        logger.info("--- Node: Initial Setup ---")
        if not state["target_info"].domain and not state["target_info"].ip_addresses:
            # This message will be seen by the LLM in the next step
            state["messages"] = add_messages(
                state["messages"],
                [
                    SystemMessage(
                        content="No target specified. Please ask the user for the primary target domain or IP address."
                    )
                ],
            )
            state["current_phase"] = "initialization"  # Stay in init to get target
        else:
            state["current_phase"] = "planning_recon"  # Move to next phase
            state["phase_objective"] = "Gather initial information about the target."
            state["messages"] = add_messages(
                state["messages"],
                [
                    SystemMessage(
                        content=f"Target identified: {state['target_info'].domain}. Moving to Planning & Reconnaissance."
                    )
                ],
            )
        return state

    def agent_think_and_act_node(self, state: WebPentestState) -> WebPentestState:
        logger.info(
            f"--- Node: Agent Think and Act (Phase: {state['current_phase']}) ---"
        )

        if state.get("requires_user_input_for_tool"):
            # Agent is paused, waiting for user input. This node shouldn't be hit directly.
            # The graph logic should route to handle_user_input_node instead.
            # This is a safeguard.
            logger.warning(
                "Agent in 'requires_user_input' state but 'think_and_act' was called. Graph logic error?"
            )
            # We should just pass through, graph's conditional edge will handle it.
            return state

        system_prompt, _ = self._get_phase_prompt_and_tools(
            state
        )  # Tools are bound to LLM separately

        # Bind tools to the LLM for this call
        llm_with_tools = self.llm.bind_tools(
            pentest_tools
        )  # Pass the actual tool objects

        current_messages = [SystemMessage(content=system_prompt)] + state["messages"][
            -5:
        ]  # Include recent history + system prompt

        ai_response_obj = llm_with_tools.invoke(current_messages)

        # Add AI's response (which could be a tool call or text) to messages
        # The AIMessage might contain `tool_calls` attribute
        state["messages"] = add_messages(state["messages"], [ai_response_obj])

        # No direct state modification here other than adding the message.
        # The next node (process_agent_response_node) will handle tool calls, phase transitions etc.
        return state

    def process_agent_response_node(self, state: WebPentestState) -> WebPentestState:
        logger.info("--- Node: Process Agent Response ---")
        last_message = state["messages"][-1]
        state["pending_tool_guidance"] = None  # Clear previous guidance

        if not isinstance(last_message, AIMessage):
            logger.warning("Last message is not AIMessage, skipping processing.")
            return state  # Should not happen if graph is correct

        ai_message_content = last_message.content

        # Check for phase transition directives
        if isinstance(ai_message_content, str):
            if ai_message_content.startswith("PHASE_OBJECTIVE_COMPLETE:"):
                next_phase_str = (
                    ai_message_content.split(":", 1)[1]
                    .strip()
                    .lower()
                    .replace(" ", "_")
                )
                if next_phase_str in WebPentestState.__annotations__["current_phase"].__args__:  # type: ignore
                    state["current_phase"] = next_phase_str  # type: ignore
                    state["phase_objective"] = (
                        None  # LLM will define new objective based on phase
                    )
                    logger.info(f"Transitioning to phase: {state['current_phase']}")
                    state["messages"] = add_messages(
                        state["messages"],
                        [
                            SystemMessage(
                                content=f"Transitioning to phase: {state['current_phase']}. Objective needs to be re-evaluated."
                            )
                        ],
                    )
                else:
                    logger.warning(
                        f"LLM tried to transition to invalid phase: {next_phase_str}"
                    )
                    state["messages"] = add_messages(
                        state["messages"],
                        [
                            SystemMessage(
                                content=f"Invalid phase transition attempted: {next_phase_str}."
                            )
                        ],
                    )
                return state  # Return to agent_think_and_act for new phase
            elif ai_message_content == "PENTEST_COMPLETE":
                state["current_phase"] = "completed"
                logger.info("Pentest marked as complete by LLM.")
                state["messages"] = add_messages(
                    state["messages"], [SystemMessage(content="Pentest complete.")]
                )
                return state  # Will go to END

        # Check for tool calls from the last AI message
        if isinstance(last_message, AIMessage) and getattr(
            last_message, "tool_calls", None
        ):
            # For simplicity, handle one tool call at a time from the LLM's perspective
            # LangGraph's ToolNode or manual iteration can handle multiple.
            # Here, we'll just take the first for this example logic.
            # The actual execution will happen in execute_tool_node.
            # This node just sets up for it.
            logger.info(f"LLM initiated tool calls: {last_message.tool_calls}")
            # No state change here regarding tool calls, that's for the conditional edge / ToolNode
        elif isinstance(ai_message_content, str) and ai_message_content:
            # This is a text response from the LLM (e.g. asking user a question, summary)
            # The message is already added to state['messages'] by agent_think_and_act_node
            logger.info(f"LLM text response: {ai_message_content[:100]}")
            # If LLM asks a question, it might imply user input is needed, but not for a *tool*
            state["requires_user_input_for_tool"] = None  # Ensure this is clear

        return state

    def _increment_tool_failure(self, state, tool_name):
        if "tool_failures" not in state:
            state["tool_failures"] = {}
        state["tool_failures"][tool_name] = state["tool_failures"].get(tool_name, 0) + 1
        return state["tool_failures"][tool_name]

    def _reset_tool_failure(self, state, tool_name):
        if "tool_failures" in state and tool_name in state["tool_failures"]:
            state["tool_failures"][tool_name] = 0

    def _get_next_phase(self, current_phase):
        phase_order = [
            "initialization",
            "planning_recon",
            "scanning",
            "gaining_access",
            "maintaining_access",
            "analysis_reporting",
            "completed",
        ]
        try:
            idx = phase_order.index(current_phase)
            if idx + 1 < len(phase_order):
                return phase_order[idx + 1]
        except Exception:
            pass
        return "completed"

    def execute_tool_node(self, state: WebPentestState) -> WebPentestState:
        logger.info("--- Node: Execute Tool ---")
        last_message = state["messages"][-1]
        tool_invocation_results = []  # To store AIMessage objects with tool_calls

        if not isinstance(last_message, AIMessage) or not getattr(
            last_message, "tool_calls", None
        ):
            logger.info("No tool calls in last AI message or last message not AI.")
            return state  # Nothing to execute

        for tool_call in last_message.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            selected_tool = tool_map.get(tool_name)

            if not selected_tool:
                result_content = f"Error: Tool '{tool_name}' not found."
                logger.error(result_content)
            else:
                logger.info(f"Executing tool: {tool_name} with args: {tool_args}")
                try:
                    observation = selected_tool.invoke(tool_args)
                    result_content = str(observation)
                    # Reset failure count on success
                    self._reset_tool_failure(state, tool_name)
                    if result_content.startswith("USER_GUIDANCE_PROVIDED_FOR::"):
                        _, tool_key, guidance_text = result_content.split("::", 2)
                        state["pending_tool_guidance"] = guidance_text
                        state["requires_user_input_for_tool"] = tool_key
                        logger.info(
                            f"Tool {tool_key} requires user input. Guidance prepared."
                        )
                        result_content = f"User guidance for {tool_key} has been prepared. Awaiting user input."
                except Exception as e:
                    result_content = f"Error executing tool {tool_name}: {e}"
                    logger.error(result_content, exc_info=True)
                    # Increment failure count
                    fail_count = self._increment_tool_failure(state, tool_name)
                    if fail_count >= 5:
                        # Skip this tool and move to next phase
                        next_phase = self._get_next_phase(state["current_phase"])
                        skip_msg = f"Tool '{tool_name}' failed {fail_count} times. Skipping and moving to next phase: {next_phase}."
                        logger.warning(skip_msg)
                        # Add to timeline
                        now = datetime.datetime.now(datetime.timezone.utc).isoformat()
                        event = {
                            "timestamp": now,
                            "phase": state["current_phase"],
                            "tool": tool_name,
                            "args": tool_args,
                            "result": skip_msg,
                            "user_id": state["user_id"],
                            "session_id": state["session_id"],
                        }
                        if "timeline" not in state or state["timeline"] is None:
                            state["timeline"] = []
                        state["timeline"].append(event)
                        state["current_phase"] = next_phase
                        state["phase_objective"] = None
                        # Optionally, add a message to the LLM
                        state["messages"] = add_messages(
                            state["messages"],
                            [SystemMessage(content=skip_msg)],
                        )
                        # Reset failure count for this tool
                        self._reset_tool_failure(state, tool_name)
                        # Stop further tool execution in this node
                        break

            tool_call_result = ToolCall(
                name=tool_name, args=tool_args, id=tool_call["id"]
            )
            ai_msg = AIMessage(content=result_content, tool_calls=[tool_call_result])
            tool_invocation_results.append(ai_msg)
            state["last_tool_raw_output"] = result_content

        valid_msgs = [m for m in tool_invocation_results if isinstance(m, BaseMessage)]
        state["messages"] = add_messages(state["messages"], valid_msgs)
        if valid_msgs:
            self._post_tool_processing(state, tool_name, tool_args, result_content)
        return state

    def _post_tool_processing(
        self,
        state: WebPentestState,
        tool_name: str,
        tool_args: Dict,
        result_content: str,
    ):
        """Helper to update state based on tool output and maintain a timeline for context."""
        logger.info(f"Post-processing for tool {tool_name} output.")
        now = datetime.datetime.now(datetime.timezone.utc).isoformat()
        event = {
            "timestamp": now,
            "phase": state["current_phase"],
            "tool": tool_name,
            "args": tool_args,
            "result": result_content,
            "user_id": state["user_id"],
            "session_id": state["session_id"],
        }
        # Update recon_findings and vulnerabilities as before
        if tool_name == "whois_lookup_tool":
            state["recon_findings"]["whois"] = result_content
        elif tool_name == "nmap_scan_tool":
            state["recon_findings"]["nmap_scan"] = result_content
            vuln = VulnerabilityInfo(
                name=f"Nmap Scan Results for {tool_args.get('target')}",
                description="Potential open ports and services found.",
                scanner_tool="Nmap",
                raw_output=result_content,
            )
            state["identified_vulnerabilities"].append(vuln)
            event["vulnerability"] = vuln.model_dump(exclude_none=True)
        # Add more tool-specific parsing as needed
        # Append event to timeline
        if "timeline" not in state or state["timeline"] is None:
            state["timeline"] = []
        state["timeline"].append(event)
        # Save event to Qdrant for semantic recall
        qdrant_db.add_entry(
            QDRANT_COLLECTION_REPORTS,  # Use a dedicated collection for timeline if desired
            json.dumps(event),
            metadata={
                "phase": state["current_phase"],
                "tool": tool_name,
                "user_id": state["user_id"],
                "session_id": state["session_id"],
                "type": "timeline_event",
                "timestamp": now,
            },
        )

    def request_user_input_node(self, state: WebPentestState) -> WebPentestState:
        logger.info("--- Node: Request User Input ---")
        # This node is reached if state['pending_tool_guidance'] is set.
        # The actual message to the user is constructed here.
        # The graph will then pause.
        guidance = state.get("pending_tool_guidance")
        tool_name_waiting = state.get("requires_user_input_for_tool")

        if guidance and tool_name_waiting:
            user_prompt_message = f"{guidance}\n\nPlease provide the results/summary for {tool_name_waiting}:"
            # This message is intended for the *human user*, not the LLM directly yet.
            # In a real application, this would be sent to the UI.
            # For this console example, we'll add it to messages to simulate it being shown.
            state["messages"] = add_messages(
                state["messages"], [AIMessage(content=user_prompt_message)]
            )  # Agent "says" this to user
            logger.info(f"Agent is now awaiting user input for {tool_name_waiting}.")
        else:
            # Should not happen if graph logic is correct
            logger.error("Request User Input node reached without pending guidance.")

        state["pending_tool_guidance"] = None  # Guidance has been delivered
        return state  # State now indicates it's waiting for user input

    def process_user_tool_input_node(self, state: WebPentestState) -> WebPentestState:
        logger.info("--- Node: Process User Tool Input ---")
        # This node is called *after* the user has provided input.
        # The user's input should be the last message in `state['messages']`.
        last_message = state["messages"][-1]
        tool_name_answered = state["requires_user_input_for_tool"]

        if isinstance(last_message, HumanMessage) and tool_name_answered:
            user_provided_data = last_message.content
            logger.info(
                f"Received user input for tool {tool_name_answered}: {user_provided_data[:100]}"
            )

            # Create a ToolMessage representing the user's manual tool execution result
            tool_message_content = (
                f"User provided results for {tool_name_answered}: {user_provided_data}"
            )
            # We need a tool_call_id. Since this wasn't a direct LLM tool_call, we can make one up or find the original AI message that requested it.
            # For simplicity, let's just state the origin.
            # This message informs the LLM about the manual step's outcome.
            state["messages"] = add_messages(
                state["messages"], [SystemMessage(content=tool_message_content)]
            )  # System informs LLM of user's action

            # TODO: Parse user_provided_data and update state (vulnerabilities, recon_findings etc.)
            # This is a critical step. E.g., if tool_name_answered == "OWASP_ZAP":
            #   Parse summary, create VulnerabilityInfo objects, add to state['identified_vulnerabilities']
            #   Store in Mem0 and Qdrant
            if tool_name_answered == "OWASP_ZAP":
                # Example: very basic parsing
                vuln = VulnerabilityInfo(
                    name=f"ZAP Scan finding (user-reported)",
                    description=user_provided_data,
                    scanner_tool="OWASP ZAP (manual)",
                )
                state["identified_vulnerabilities"].append(vuln)
                mem0_storage.add(
                    data={"tool": "OWASP_ZAP", "user_summary": user_provided_data},
                    user_id=state["user_id"],
                    session_id=state["session_id"],
                    metadata={"phase": state["current_phase"]},
                )
                qdrant_db.add_entry(
                    QDRANT_COLLECTION_VULNS,
                    f"ZAP (user summary): {user_provided_data[:500]}",
                    metadata={"tool": "OWASP_ZAP"},
                )

            state["requires_user_input_for_tool"] = None  # Clear the flag
            state["last_tool_raw_output"] = (
                user_provided_data  # Store user's input as "tool output"
            )
        else:
            logger.warning(
                "Process User Tool Input node called without valid preceding user message."
            )

        return state  # Return to agent_think_and_act

    def generate_report_node(self, state: WebPentestState) -> WebPentestState:
        logger.info("--- Node: Generate Report ---")
        # Use the timeline for a detailed report
        timeline_summary = "\n".join(
            [
                f"[{e.get('timestamp', '')}] Phase: {e.get('phase', '')}, Tool: {e.get('tool', '')}, Result: {str(e.get('result', ''))[:200]}..."
                for e in state.get("timeline", [])
            ]
        )
        summary = "Final Report (Timeline-based):\n"
        summary += f"Target: {state['target_info'].model_dump_json(indent=2)}\n"
        summary += f"Recon Findings: {json.dumps(state['recon_findings'], indent=2)}\n"
        summary += f"Vulnerabilities: {[v.model_dump_json() for v in state['identified_vulnerabilities']]}\n"
        summary += f"Access Points: {[ap.model_dump_json() for ap in state['gained_access_points']]}\n"
        summary += f"Persistence: {[p.model_dump_json() for p in state['persistence_mechanisms']]}\n"
        summary += f"\n--- Timeline of Actions ---\n{timeline_summary}\n"

        state["report_data_summary"] = summary
        state["messages"] = add_messages(
            state["messages"], [AIMessage(content=summary)]
        )
        logger.info("Final report (timeline-based) generated.")

        # Save report to Mem0 and Qdrant
        mem0_storage.add(
            data={"report_summary": summary},
            user_id=state["user_id"],
            session_id=state["session_id"],
            metadata={
                "memory_key": f"final_report_{state['target_info'].domain or 'unknown_target'}",
                "type": "final_report",
            },
        )
        qdrant_db.add_entry(
            QDRANT_COLLECTION_REPORTS,
            summary,
            metadata={"target": state["target_info"].domain or "unknown_target"},
        )

        state["current_phase"] = "completed"  # Ensure phase is completed
        return state

    # --- Graph Construction ---
    def _build_graph(self) -> StateGraph:
        graph = StateGraph(WebPentestState)

        graph.add_node("initial_setup", self.initial_setup_node)
        graph.add_node("agent_think_and_act", self.agent_think_and_act_node)
        graph.add_node("process_agent_response", self.process_agent_response_node)
        graph.add_node("execute_tool", self.execute_tool_node)
        graph.add_node("request_user_input", self.request_user_input_node)
        graph.add_node("process_user_tool_input", self.process_user_tool_input_node)
        graph.add_node("generate_report", self.generate_report_node)

        graph.set_entry_point("initial_setup")
        graph.add_edge("initial_setup", "agent_think_and_act")

        graph.add_edge("agent_think_and_act", "process_agent_response")

        # Conditional Edges from Process Agent Response
        graph.add_conditional_edges(
            "process_agent_response",
            # Decision function:
            lambda state: (
                "execute_tool"
                if isinstance(state["messages"][-1], AIMessage)
                and getattr(state["messages"][-1], "tool_calls", None)
                else (
                    "agent_think_and_act"
                    if state["current_phase"] not in ["analysis_reporting", "completed"]
                    else (
                        "generate_report"
                        if state["current_phase"] == "analysis_reporting"
                        else END
                    )
                )
            ),
            # Path map:
            {
                "execute_tool": "execute_tool",
                "agent_think_and_act": "agent_think_and_act",  # Loop back to think if no tool call and not reporting/done
                "generate_report": "generate_report",
                END: END,
            },
        )

        # After tool execution
        graph.add_conditional_edges(
            "execute_tool",
            # Decision function:
            lambda state: (
                "request_user_input"
                if state.get("requires_user_input_for_tool")
                else "agent_think_and_act"
            ),
            # Path map:
            {
                "request_user_input": "request_user_input",  # Go to ask user
                "agent_think_and_act": "agent_think_and_act",  # Go back to LLM to process tool output
            },
        )

        # After user input is requested (graph pauses here in a real setup)
        # For this example, we assume user input is appended to messages, then this node is triggered
        graph.add_edge(
            "request_user_input", "process_user_tool_input"
        )  # This edge implies a pause for actual user interaction
        graph.add_edge(
            "process_user_tool_input", "agent_think_and_act"
        )  # Back to LLM after user provides data

        graph.add_edge("generate_report", END)

        return graph

    def invoke(
        self, human_query: Optional[str] = None, initial_target: Optional[str] = None
    ):
        """
        Invokes the agent.
        If human_query is provided, it's added as the first user message.
        The graph will pause if it reaches 'request_user_input_node' and requires_user_input_for_tool is set.
        The caller is then responsible for getting user input and reinvoking with that input.
        """
        current_state = (
            self.initial_state.copy()
        )  # Start with a fresh copy of initial state
        if initial_target and not current_state["target_info"].domain:
            current_state["target_info"].domain = initial_target
            logger.info(f"Initial target set to: {initial_target}")

        if human_query:
            current_state["messages"] = add_messages(
                current_state["messages"], [HumanMessage(content=human_query)]
            )

        logger.info(
            f"Invoking WebPentestAgent with initial state: {current_state['current_phase']}"
        )

        # Simulate the run, stopping if user input is required
        final_loop_state = None
        for s in self.agent_runnable.stream(current_state, {"recursion_limit": 50}):
            # The 's' here is the full state dict at each step.
            # We are interested in the state of the last completed node.
            # The key of the dict 's' will be the node name.
            node_name = list(s.keys())[0]
            current_loop_state = s[node_name]
            final_loop_state = current_loop_state  # Keep track of the latest state

            logger.info(f"--- Completed Node: {node_name} ---")
            logger.debug(
                f"State after {node_name}: RequiresUserInput='{current_loop_state.get('requires_user_input_for_tool')}', Phase='{current_loop_state.get('current_phase')}'"
            )
            # logger.debug(f"Messages: {current_loop_state['messages']}")

            if node_name == "request_user_input" and current_loop_state.get(
                "requires_user_input_for_tool"
            ):
                logger.info(
                    f"Agent paused, awaiting user input for tool: {current_loop_state['requires_user_input_for_tool']}"
                )
                # The last AI message contains the prompt for the user
                user_prompt = ""
                for msg in reversed(current_loop_state["messages"]):
                    if isinstance(msg, AIMessage):
                        user_prompt = msg.content
                        break
                print(f"\n>>> AGENT TO USER:\n{user_prompt}")
                return current_loop_state  # Return current state so caller can handle input

            if (
                node_name == END
                or current_loop_state.get("current_phase") == "completed"
            ):
                logger.info("Agent run completed or reached END node.")
                break

        return final_loop_state


# --- Example Usage ---
if __name__ == "__main__":
    USER_ID = "test_user_001"
    SESSION_ID = f"pentest_session_{uuid.uuid4()}"

    # Ensure OPENAI_API_KEY is set in your environment
    if not os.getenv("OPENAI_API_KEY"):
        print("FATAL: OPENAI_API_KEY not set. Please set it to run the example.")
        exit()
    if not qdrant_db or not qdrant_db.client:  # type: ignore
        print("Warning: Qdrant not available. Some features will be skipped.")

    # Example: Pre-populate Qdrant with some dummy payload
    if qdrant_db and qdrant_db.client:  # type: ignore
        qdrant_db.add_entry(
            QDRANT_COLLECTION_PAYLOADS,
            "Example SQL Injection Payload: ' OR 1=1 --",
            {"type": "SQLi", "description": "Basic tautology for SQL injection."},
        )
        qdrant_db.add_entry(
            QDRANT_COLLECTION_PAYLOADS,
            "Example XSS Payload: <script>alert('XSS')</script>",
            {"type": "XSS", "description": "Simple XSS alert."},
        )

    # --- Interactive Loop ---
    print("Initializing Web Pentest Agent...")
    # Create agent instance for the session
    # Example target for testing: testphp.vulnweb.com (intentionally vulnerable)
    # Or use a domain you have explicit permission to test.
    agent = WebPentestAgent(user_id=USER_ID, session_id=SESSION_ID)

    current_pentest_state = agent.initial_state.copy()

    print(f"Web Pentest Agent ready. Target can be set via initial prompt.")
    print("Type 'exit' to quit.")

    # Initial prompt to the agent (e.g., to set target or start process)
    initial_human_input = input(f"\n[User {USER_ID}] >>> ")
    if initial_human_input.lower() == "exit":
        exit()

    # Add initial human message to the state before the first invoke
    current_pentest_state["messages"] = add_messages(
        current_pentest_state["messages"], [HumanMessage(content=initial_human_input)]
    )

    while True:
        returned_state = agent.agent_runnable.invoke(
            current_pentest_state, {"recursion_limit": 50}
        )
        # After invoke, LangGraph internally updates current_pentest_state if using checkpointer.
        # Without checkpointer, invoke returns the final state of that run.
        # For streaming/interactive, we need to manage state turn by turn.

        # For this loop, let's use the returned state to continue
        if not returned_state:  # Should not happen with invoke
            logger.error("Agent invocation returned None. Exiting.")
            break

        current_pentest_state = returned_state  # Update state for the next turn

        # Check if agent is waiting for user input for a tool
        if current_pentest_state.get("requires_user_input_for_tool"):
            tool_waiting_for = current_pentest_state["requires_user_input_for_tool"]
            # The prompt to the user should be the last AI message
            user_guidance_prompt = "Guidance not found."
            for msg in reversed(
                current_pentest_state["messages"]
            ):  # Find last AI message
                if isinstance(msg, AIMessage):
                    user_guidance_prompt = msg.content
                    break

            print(
                f"\n>>> AGENT (awaiting input for {tool_waiting_for}):\n{user_guidance_prompt}"
            )

            human_tool_input = input(
                f"\n[User {USER_ID} - Input for {tool_waiting_for}] >>> "
            )
            if human_tool_input.lower() == "exit":
                break

            # Add user's tool input to messages and clear the waiting flag
            current_pentest_state["messages"] = add_messages(
                current_pentest_state["messages"],
                [HumanMessage(content=human_tool_input)],
            )
            # The process_user_tool_input_node will handle clearing 'requires_user_input_for_tool'
            # when the graph runs next.

        # Check if agent has finished or provided a conversational response
        elif current_pentest_state.get("current_phase") == "completed":
            print("\n>>> AGENT: Penetration test process completed.")
            final_report_msg = "No final report generated in messages."
            for msg in reversed(current_pentest_state["messages"]):
                if isinstance(msg, AIMessage) and "Final Report" in msg.content:
                    final_report_msg = msg.content
                    break
            print(final_report_msg)
            break
        else:
            # Agent might have provided a conversational response not requiring tool input
            last_ai_msg_content = "No conversational AI response."
            for msg in reversed(current_pentest_state["messages"]):
                if isinstance(msg, AIMessage) and not getattr(
                    msg, "tool_calls", None
                ):  # A text response, not tool call
                    last_ai_msg_content = msg.content
                    break

            print(f"\n>>> AGENT:\n{last_ai_msg_content}")

            # General prompt for next user action if not waiting for specific tool input
            next_human_input = input(f"\n[User {USER_ID}] >>> ")
            if next_human_input.lower() == "exit":
                break
            current_pentest_state["messages"] = add_messages(
                current_pentest_state["messages"],
                [HumanMessage(content=next_human_input)],
            )
